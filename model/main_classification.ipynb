{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from classification import determine_direction\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from mlp_classification import NGCCPHAT\n",
    "from dataset_classification import STACKED_dx_dy\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Set the backend to TkAgg, or choose another appropriate backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "# For reproducibility\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "runs = os.listdir('../runs')\n",
    "run_number = len(runs)\n",
    "run_path = f'../runs/run_{run_number}'\n",
    "os.mkdir(run_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining all the parameters\n",
    "'''\n",
    "data_path = r\"..\\data\\training_data_sample_1.pkl\"\n",
    "print('Loading data...')\n",
    "data = pd.read_pickle(data_path)\n",
    "\n",
    "# Training hyperparams\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "lr = 0.001             # learning rate\n",
    "wd = 0.01              # weight decay\n",
    "patience = 5           # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "# Model parameters\n",
    "max_tau = 60           # maximum tau value for GCC-PHAT\n",
    "num_channels = 10       # number of channels in final layer of NGCCPHAT backbone\n",
    "conv_channels = 15     # number of channels in the convolutional layers of NGCCPHAT backbone\n",
    "fs = 204800            # sampling rate\n",
    "number_of_stacked = 40 # number of stacked snippets\n",
    "n_outputs = 4          # number of kvadrants classification\n",
    "\n",
    "\n",
    "sincnet_params = {'input_dim': len(data.sensor_1[0]),\n",
    "                          'fs': fs,\n",
    "                          'cnn_N_filt': [20,   num_channels],\n",
    "                          'cnn_len_filt': [1023,  7],\n",
    "                          'cnn_max_pool_len': [1, 1],\n",
    "                          'cnn_act': ['leaky_relu', 'linear'],\n",
    "                          'cnn_drop': [0.0, 0.0],\n",
    "                          'max_hz': 20000.0,\n",
    "                          'low_hz': 1000.0,\n",
    "                          'min_band_hz': 1000.0,\n",
    "                          }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum value of dx and dy is: 0.6466163396650586\n",
      "Plotting data distribution...\n",
      "Training set size: 64\n",
      "Validation set size: 16\n"
     ]
    }
   ],
   "source": [
    "# Normalising the dx and dy values\n",
    "max = data[[\"dx\", \"dy\"]].abs().max().max()\n",
    "print(f\"The maximum value of dx and dy is: {max}\")\n",
    "data[\"dx\"] = data[\"dx\"]/max\n",
    "data[\"dy\"] = data[\"dy\"]/max\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "training_data, validation_data = train_test_split(data, test_size=0.2, random_state=seed, shuffle=True,)\n",
    "\n",
    "print('Plotting data distribution...')\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "ax[0].hist(training_data['dx'], bins=100, alpha=0.5, label='Training')\n",
    "ax[0].hist(validation_data['dx'], bins=100, alpha=0.5, label='Validation')\n",
    "ax[0].set_title('dx distribution')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(training_data['dy'], bins=100, alpha=0.5, label='Training')\n",
    "ax[1].hist(validation_data['dy'], bins=100, alpha=0.5, label='Validation')\n",
    "ax[1].set_title('dy distribution')  \n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].scatter(training_data['dx'], training_data['dy'], alpha=0.5, label='Training')\n",
    "ax[2].scatter(validation_data['dx'], validation_data['dy'], alpha=0.5, label='Validation')  \n",
    "ax[2].set_title('dx vs dy')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[0].set_xlim(-1, 1)\n",
    "ax[1].set_xlim(-1, 1)\n",
    "ax[2].set_xlim(-1, 1)\n",
    "ax[2].set_ylim(-1,1)\n",
    "\n",
    "plt.savefig(f'{run_path}/data_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "train_set = STACKED_dx_dy(training_data, number_of_stacked, n_outputs)\n",
    "val_set = STACKED_dx_dy(validation_data, number_of_stacked, n_outputs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False   , drop_last=True)\n",
    "\n",
    "print(f'Training set size: {len(train_set)}')\n",
    "print(f'Validation set size: {len(val_set)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SincNet initialized\n",
      "Sinconv_fast initialized\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m NGCCPHAT(max_tau, num_channels, conv_channels, sincnet_params,number_of_stacked, n_outputs)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd)\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\malan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:307\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = NGCCPHAT(max_tau, num_channels, conv_channels, sincnet_params,number_of_stacked, n_outputs)\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Before training starts\n",
    "initial_low_hz =  model.backbone.conv[0].low_hz_.data.clone()\n",
    "initial_band_hz = model.backbone.conv[0].band_hz_.data.clone()\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss_epoch = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x1, x2, x3, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {e+1}/{epochs} Training\")):\n",
    "        x1 = x1.cuda()\n",
    "        x2 = x2.cuda()\n",
    "        x3 = x3.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "\n",
    "        predicted = model(x1, x2, x3)\n",
    "        loss = loss_fn(predicted, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "    training_loss.append(train_loss_epoch / len(train_loader))\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0\n",
    "    for batch_idx, (x1, x2, x3, target) in enumerate(tqdm(val_loader, desc=f\"Epoch {e+1}/{epochs} Validation\")):\n",
    "        with torch.no_grad():\n",
    "            x1 = x1.cuda()\n",
    "            x2 = x2.cuda()\n",
    "            x3 = x3.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "            predicted = model(x1, x2, x3)\n",
    "            loss = loss_fn(predicted, target)\n",
    "            val_loss_epoch += loss.item()\n",
    "\n",
    "    current_val_loss = val_loss_epoch / len(val_loader)\n",
    "    validation_loss.append(current_val_loss)\n",
    "    print(f'Epoch {e+1}/{epochs} - Training Loss: {training_loss[-1]:.4f} - Validation Loss: {validation_loss[-1]:.4f}')\n",
    "\n",
    "    delta_low_hz = torch.abs(initial_low_hz - model.backbone.conv[0].low_hz_.data)\n",
    "    delta_band_hz = torch.abs(initial_band_hz - model.backbone.conv[0].band_hz_.data)\n",
    "    print(f\"Epoch {e}: Δ Low Hz = {delta_low_hz.mean().item()}, Δ Band Hz = {delta_band_hz.mean().item()}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Optional: Save model here if current validation loss has improved\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered after epoch {e+1}. No improvement in validation loss for {patience} consecutive epochs.')\n",
    "            break  # Break out of the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), f'{run_path}/model.pth')\n",
    "torch.save(optimizer.state_dict(), f'{run_path}/optimizer.pth')\n",
    "torch.save(scheduler.state_dict(), f'{run_path}/scheduler.pth')\n",
    "\n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(training_loss, label='Training Loss')\n",
    "ax.plot(validation_loss, label='Validation Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "plt.savefig(f'{run_path}/loss.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# 3. Confusion matrix\n",
    "p_predicted = []\n",
    "p_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x1, x2, x3, labels) in enumerate(tqdm(val_loader, desc=\"Predicting\")):\n",
    "        x1 = x1.cuda()\n",
    "        x2 = x2.cuda()\n",
    "        x3 = x3.cuda()\n",
    "        labels = labels.cuda()\n",
    "         \n",
    "        outputs = model(x1,x2,x3)\n",
    "        p_predicted.append(outputs.cpu().numpy())\n",
    "        p_true.append(      labels.cpu().numpy())\n",
    "\n",
    "p_predicted = np.concatenate(p_predicted, axis=0)\n",
    "p_true = np.concatenate(p_true, axis=0)\n",
    "\n",
    "p_predicted = np.argmax(p_predicted, axis=1)\n",
    "p_true = np.argmax(p_true, axis=1) \n",
    "\n",
    "cm = confusion_matrix(p_true, p_predicted) \n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='viridis', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "\n",
    "ax.set_title('Confusion matrix')\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'{run_path}/confusion_matrix.png')\n",
    "\n",
    "\n",
    "\n",
    "# Creating cross correlation plots\n",
    "cc_path = f'{run_path}/cross_correlation_plots'\n",
    "os.mkdir(cc_path)\n",
    "\n",
    "filters = model.backbone.conv[0].filters.detach().cpu().numpy()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x1, x2, x3, labels) in enumerate(tqdm(val_loader, desc=\"Predicting\")):\n",
    "        cc = model.create_gcc(x1, x2, x3)\n",
    "        cc = cc.detach().cpu().numpy()\n",
    "        for batch in range(cc.shape[0]):\n",
    "            for stack in range(cc.shape[1]):\n",
    "                fig, ax = plt.subplots(2, 1, figsize=(10, 5))\n",
    "                ax = ax.ravel()\n",
    "                ax[0].imshow(cc[batch, stack, :, :])\n",
    "                ax[0].set_title(f'GCC for Filter nr {stack}')\n",
    "                ax[0].set_aspect('auto')\n",
    "                ax[0].set_xlabel('Tau')\n",
    "                ax[0].set_ylabel('Time [s]')\n",
    "\n",
    "                ax[1].plot(filters[stack, 0, :])\n",
    "                ax[1].set_title('Filter')\n",
    "                ax[1].set_xlabel('Time [s]')\n",
    "                ax[1].set_ylabel('Amplitude')\n",
    "                fig.tight_layout()\n",
    "                plt.savefig(f'{cc_path}/GCC_for_filter_nr_{stack}.png')\n",
    "                plt.close() \n",
    "            break\n",
    "\n",
    "\n",
    "# Calculating the frequency response of the filters \n",
    "freq_path = f'{run_path}/frequency_response'\n",
    "os.mkdir(freq_path)\n",
    "\n",
    "freq_response = []\n",
    "sampling_rate = 204800\n",
    "for i in range(filters.shape[0]):\n",
    "    freq_response.append(np.abs(np.fft.rfft(filters[i, 0, :])))\n",
    "\n",
    "freq = np.fft.rfftfreq(filters.shape[2], d=1/sampling_rate)\n",
    "for i in range(len(freq_response)):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "    ax.plot(freq, 20*np.log10(freq_response[i]))\n",
    "    ax.set_title(f'Filter {i}')\n",
    "    ax.set_xlabel('Frequency [Hz]')\n",
    "    ax.set_ylabel('Amplitude [dB]')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{freq_path}/filter_{i}_frequency_response.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'data_path': data_path,\n",
    "    'seed': seed,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'wd': wd,\n",
    "    'max_tau': max_tau,\n",
    "    'num_channels': num_channels,\n",
    "    'conv_channels': conv_channels,\n",
    "    'fs': fs,\n",
    "    'sincnet_params': sincnet_params,\n",
    "    'patience': patience,\n",
    "    'number_of_stacked': number_of_stacked,\n",
    "    'n_outputs': n_outputs,\n",
    "    'training_set_size': len(train_set),\n",
    "    'validation_set_size': len(val_set),\n",
    "    'best_val_loss': best_val_loss,\n",
    "}\n",
    "\n",
    "with open(f'{run_path}/parameters.txt', 'w') as f:\n",
    "    for key, value in parameters.items():\n",
    "        f.write(f'{key}: {value}\\n')\n",
    "\n",
    "\n",
    "# Saving the loss in a numpy file\n",
    "np.save(f'{run_path}/training_loss.npy', np.array(training_loss))\n",
    "np.save(f'{run_path}/validation_loss.npy', np.array(validation_loss))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
